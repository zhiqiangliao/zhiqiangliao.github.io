<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://zhiqiangliao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zhiqiangliao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-12T23:44:09+03:00</updated><id>https://zhiqiangliao.github.io/feed.xml</id><title type="html">blank</title><subtitle>A personal academic website, and the copy right belongs to zhiqiang liao. </subtitle><entry><title type="html">Why do I write blog?</title><link href="https://zhiqiangliao.github.io/blog/2199/intro/" rel="alternate" type="text/html" title="Why do I write blog?"/><published>2199-01-01T00:00:00+02:00</published><updated>2199-01-01T00:00:00+02:00</updated><id>https://zhiqiangliao.github.io/blog/2199/intro</id><content type="html" xml:base="https://zhiqiangliao.github.io/blog/2199/intro/"><![CDATA[<p>I think that writing a blog helps store my thoughts that may be hard to be published. But it is not trivial; a blog works like an encyclopedia that preserves insights and technical details. Also, I want to share my experience and feelings out there. I hope it helps others as well.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction for the future]]></summary></entry><entry><title type="html">Parallel computing on HPC clusters</title><link href="https://zhiqiangliao.github.io/blog/2023/parallel/" rel="alternate" type="text/html" title="Parallel computing on HPC clusters"/><published>2023-03-05T00:00:00+02:00</published><updated>2023-03-05T00:00:00+02:00</updated><id>https://zhiqiangliao.github.io/blog/2023/parallel</id><content type="html" xml:base="https://zhiqiangliao.github.io/blog/2023/parallel/"><![CDATA[<h1 id="parallel-computing-with-julia">Parallel computing with Julia</h1> <p><a href="https://julialang.org/">Julia</a> has become a popular programming language. In this article, I will use <code class="language-plaintext highlighter-rouge">multi-threading</code> both on multi-core PCs and HPC clusters.</p> <p>First, I give an example of parallel computing with for loop:</p> <figure class="highlight"><pre><code class="language-julia" data-lang="julia"><span class="c"># Julia code</span>
<span class="k">using</span> <span class="n">CSV</span><span class="x">,</span> <span class="n">DataFrames</span><span class="x">,</span> <span class="n">Base</span><span class="o">.</span><span class="n">Threads</span><span class="x">,</span> <span class="n">BenchmarkTools</span>

<span class="n">M</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">sig</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">error</span> <span class="o">=</span> <span class="n">zeros</span><span class="x">(</span><span class="n">M</span><span class="x">)</span>
<span class="nd">@btime</span> <span class="nd">@threads</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">M</span>
    <span class="n">error</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">=</span> <span class="n">simulation</span><span class="x">(</span><span class="n">n</span><span class="x">,</span> <span class="n">d</span><span class="x">,</span> <span class="n">sig</span><span class="x">)</span>
<span class="k">end</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="x">(</span><span class="n">error</span><span class="x">,</span> <span class="o">:</span><span class="n">auto</span><span class="x">)</span>

<span class="n">CSV</span><span class="o">.</span><span class="n">write</span><span class="x">(</span><span class="s">"measure</span><span class="si">$(n)</span><span class="s">_</span><span class="si">$(d)</span><span class="s">_</span><span class="si">$(sig)</span><span class="s">.csv"</span><span class="x">,</span> <span class="n">df</span><span class="x">)</span></code></pre></figure> <p>In this example, we implement 50 Monte Calo simulations with function <code class="language-plaintext highlighter-rouge">simulation()</code> in parallel. <code class="language-plaintext highlighter-rouge">@btime</code> a benmark to print out the runtime of the for loop.</p> <p>On a multi-core PC, we can run the Julia file <code class="language-plaintext highlighter-rouge">code.jl</code> with 8 threads as follows</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash">julia <span class="nt">-t</span> 8 code.jl</code></pre></figure> <p>On a HPC cluster, we can run Julia file with 8 threads as follows</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#!/bin/bash</span>
<span class="c">#SBATCH -p batch</span>
<span class="c">#SBATCH -t 00:10:00</span>
<span class="c">#SBATCH -N 1       </span>
<span class="c">#SBATCH -n 1     </span>
<span class="c">#SBATCH --cpus-per-task=8</span>
<span class="c">#SBATCH --mem-per-cpu=4G</span>
<span class="c">#SBATCH --output=code_%A.out</span>
<span class="c">#SBATCH--mail-type=FAIL</span>
<span class="c">#SBATCH--mail-type=END</span>
<span class="c">#SBATCH--mail-user=xxxx@gmail.com</span>

<span class="nb">echo</span> <span class="s2">"SLURM_JOBID: "</span> <span class="nv">$SLURM_JOBID</span>
<span class="nb">echo</span> <span class="s2">"SLURM_ARRAY_TASK_ID: "</span> <span class="nv">$SLURM_ARRAY_TASK_ID</span>
<span class="nb">echo</span> <span class="s2">"SLURM_ARRAY_JOB_ID: "</span> <span class="nv">$SLURM_ARRAY_JOB_ID</span>

module load julia
<span class="c"># Run Julia</span>
srun julia <span class="nt">-t</span> <span class="nv">$SLURM_CPUS_PER_TASK</span> code.jl</code></pre></figure> <p>In this <code class="language-plaintext highlighter-rouge">slurm</code> file, please modify <code class="language-plaintext highlighter-rouge">xxxx@gmail.com</code> to your email address where you can receive a notification when the computation is done.</p> <hr/> <h1 id="parallel-computing-with-python">Parallel computing with Python</h1> <p><a href="https://www.python.org/">Python</a> is also a very popular programming language. In the example, I will use <code class="language-plaintext highlighter-rouge">multi-processing</code> on HPC clusters. On multi-core PCs, we can simply set <code class="language-plaintext highlighter-rouge">cpus = 4</code> in the Python file.</p> <p>First, I give an example of parallel computing with <code class="language-plaintext highlighter-rouge">map</code> function:</p> <figure class="highlight"><pre><code class="language-julia" data-lang="julia"><span class="k">import</span> <span class="n">numpy</span> <span class="n">as</span> <span class="n">np</span>
<span class="k">import</span> <span class="n">pandas</span> <span class="n">as</span> <span class="n">pd</span>
<span class="k">import</span> <span class="n">random</span>
<span class="k">import</span> <span class="n">os</span>
<span class="n">from</span> <span class="n">multiprocessing</span> <span class="k">import</span> <span class="n">Pool</span>

<span class="k">if</span> <span class="err">'</span><span class="n">SLURM_CPUS_PER_TASK</span><span class="err">'</span> <span class="k">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">:</span>
    <span class="n">cpus</span> <span class="o">=</span> <span class="n">int</span><span class="x">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="x">[</span><span class="err">'</span><span class="n">SLURM_CPUS_PER_TASK</span><span class="err">'</span><span class="x">])</span>
    <span class="n">print</span><span class="x">(</span><span class="s">"Dectected %s CPUs through slurm"</span><span class="o">%</span><span class="n">cpus</span><span class="x">)</span>
<span class="k">else</span><span class="o">:</span>
    <span class="c"># None means that it will auto-detect based on os.cpu_count()</span>
    <span class="n">cpus</span> <span class="o">=</span> <span class="n">None</span>
    <span class="n">print</span><span class="x">(</span><span class="s">"Running on default number of CPUs (default: all=%s)"</span><span class="o">%</span><span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="x">())</span>

<span class="n">def</span> <span class="n">process</span><span class="x">(</span><span class="n">arg</span><span class="x">)</span><span class="o">:</span>

    <span class="n">mse</span> <span class="o">=</span> <span class="n">simulation</span><span class="x">(</span><span class="n">n</span><span class="x">,</span> <span class="n">d</span><span class="x">,</span> <span class="n">sig</span><span class="x">)</span>
    <span class="k">return</span> <span class="n">mse</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="err">'</span><span class="n">__main__</span><span class="err">'</span><span class="o">:</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="x">(</span><span class="mi">0</span><span class="x">)</span>
    <span class="n">n</span><span class="o">=</span><span class="mi">100</span>
    <span class="n">d</span><span class="o">=</span><span class="mi">3</span>
    <span class="n">sig</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">with</span> <span class="n">Pool</span><span class="x">(</span><span class="n">cpus</span><span class="x">)</span> <span class="n">as</span> <span class="n">p</span><span class="o">:</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="x">(</span><span class="n">p</span><span class="o">.</span><span class="n">map</span><span class="x">(</span><span class="n">process</span><span class="x">,</span> <span class="n">range</span><span class="x">(</span><span class="mi">50</span><span class="x">)))</span>
        <span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="x">(</span><span class="err">'</span><span class="n">mse</span><span class="err">'</span> <span class="o">+</span> <span class="err">'</span><span class="x">{</span><span class="mi">0</span><span class="x">}</span><span class="n">_</span><span class="x">{</span><span class="mi">1</span><span class="x">}</span><span class="n">_</span><span class="x">{</span><span class="mi">2</span><span class="x">}</span><span class="o">.</span><span class="n">csv</span><span class="err">'</span><span class="o">.</span><span class="n">format</span><span class="x">(</span><span class="n">n</span><span class="x">,</span> <span class="n">d</span><span class="x">,</span> <span class="n">sig</span><span class="x">))</span></code></pre></figure> <p>In this example, we implement 50 Monte Calo simulations with function <code class="language-plaintext highlighter-rouge">simulation()</code> in parallel.</p> <p>On a HPC cluster, we can run Python file as follows</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#!/bin/bash</span>
<span class="c">#SBATCH -p batch</span>
<span class="c">#SBATCH -t 00:10:00</span>
<span class="c">#SBATCH -N 1       </span>
<span class="c">#SBATCH -n 1     </span>
<span class="c">#SBATCH --cpus-per-task=8</span>
<span class="c">#SBATCH --mem-per-cpu=4G</span>
<span class="c">#SBATCH --output=code_%A.out</span>
<span class="c">#SBATCH--mail-type=FAIL</span>
<span class="c">#SBATCH--mail-type=END</span>
<span class="c">#SBATCH--mail-user=xxxx@gmail.com</span>

<span class="nb">echo</span> <span class="s2">"SLURM_JOBID: "</span> <span class="nv">$SLURM_JOBID</span>
<span class="nb">echo</span> <span class="s2">"SLURM_ARRAY_TASK_ID: "</span> <span class="nv">$SLURM_ARRAY_TASK_ID</span>
<span class="nb">echo</span> <span class="s2">"SLURM_ARRAY_JOB_ID: "</span> <span class="nv">$SLURM_ARRAY_JOB_ID</span>

module purge
module load anaconda
<span class="c"># Run Python</span>
srun python3 code.py</code></pre></figure> <hr/>]]></content><author><name>Zhiqiang Liao</name></author><category term="Research"/><category term="code"/><category term="github"/><summary type="html"><![CDATA[工欲善其事，必先利其器]]></summary></entry><entry><title type="html">Notes on Norms</title><link href="https://zhiqiangliao.github.io/blog/2022/norm/" rel="alternate" type="text/html" title="Notes on Norms"/><published>2022-11-24T22:30:16+02:00</published><updated>2022-11-24T22:30:16+02:00</updated><id>https://zhiqiangliao.github.io/blog/2022/norm</id><content type="html" xml:base="https://zhiqiangliao.github.io/blog/2022/norm/"><![CDATA[<p>(1) L0 norm (\(\|\theta\|_0\))</p> <p>The zero norm (also known as \(l_0\) norm, L0 norm) of a vector \(\theta \in \{1,...,d\}\) is denoted \(\|\theta\|_0\) and is defined as follows:</p> <p>\begin{equation} \label{eq: l0} \lVert\theta\lVert_0 = \sum_{j=1}^d \mathbb{I}(\theta_j \neq 0) \end{equation}</p> <p>where \(\mathbb{I}(\cdot)\) is an <a href="https://en.wikipedia.org/wiki/Indicator_function">indicator function</a>. Thus, L0 norm indicate the number of nonzero elements in \(\theta\).</p> <p>(2) L1 norm (\(\|\theta\|_1\))</p> <p>The one norm (also known as the \(l_1\) norm, L1 norm, or mean norm) of a vector \(\theta \in \{1,...,d\}\) is denoted \(\|\theta\|_1\) and is defined as follows:</p> <p>\begin{equation} \label{eq: l1} \lVert\theta\lVert_1 = \sum_{j=1}^d \lvert\theta_j\lvert \end{equation}</p> <p>Thus, L1 norm is defined as the sum of the absolute values of its elements.</p> <p>(3) L2 norm (\(\|\theta\|_2\))</p> <p>The two norm (also known as the \(l_2\) norm, L2 norm, or mean-square norm, least squares norm) of a vector \(\theta \in \{1,...,d\}\) is denoted \(\|\theta\|_2\) and is defined as follows:</p> <p>\begin{equation} \label{eq: l2} \lVert\theta\lVert_2 = \sqrt{\sum_{j=1}^d \lvert\theta_j\lvert^2} \end{equation}</p> <p>L2 norm is defined as the square root of the sum of the squares of the absolute values of its elements.</p> <p>(4) Infinity norm (\(\|\theta\|_{\infty}\))</p> <p>The infinity norm (also known as the \(l_{\infty}\) norm, max norm, or uniform norm) of a vector \(\theta \in \{1,...,d\}\) is denoted \(\|\theta\|_2\) and is defined as follows:</p> \[\begin{equation} \label{eq: linf} \|\theta\|_{\infty} = \max_{j=1,...,d} \{|\theta_j|\} \end{equation}\] <p>\(L_{\infty}\) norm is defined as the maximum of the absolute values of its elements.</p> <p>(5) Lp norm (\(\|\theta\|_{p}\))</p> <p>The p norm (also known as the \(l_p\) norm, Lp norm) of a vector \(\theta \in \{1,...,d\}\) is denoted \(\|\theta\|_p\) and is defined as follows:</p> <p>\begin{equation} \label{eq: lp} \lVert\theta\lVert_p = \sqrt[p]{\sum_{j=1}^d \lvert\theta_j\lvert^p} \end{equation}</p>]]></content><author><name></name></author><category term="Research"/><category term="math"/><summary type="html"><![CDATA[Statistics]]></summary></entry><entry><title type="html">Cross Validation in Machine Learning</title><link href="https://zhiqiangliao.github.io/blog/2022/cv/" rel="alternate" type="text/html" title="Cross Validation in Machine Learning"/><published>2022-11-18T00:00:00+02:00</published><updated>2022-11-18T00:00:00+02:00</updated><id>https://zhiqiangliao.github.io/blog/2022/cv</id><content type="html" xml:base="https://zhiqiangliao.github.io/blog/2022/cv/"><![CDATA[<h1 id="cross-validation">Cross validation</h1> <p>Cross validation becomes an increasingly popular method in either theoretical or practical research. There are usually two types of cross validation techniques. 1) The first is to provide a statistical analysis about how accurately a predictive model will perform in practice. 2) The second is to find the optimal hyperparameter or tune hyperparameter. They usually refer to <code class="language-plaintext highlighter-rouge">model evaluation</code> and <code class="language-plaintext highlighter-rouge">model selection</code>, respectively.</p> <p>I think it is important to clarify the difference between these two types of cross validation techniques. That always confuses beginners, as it confused me when I was learning this technique. I would say the <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">Wikipedia article</a> is not good enough for beginners. Wikipedia refers the second one as nested cross-validation, but it is obviously not clear.</p> <p>In effect, the basic idea of them is the same: out-of-sample testing. Next, I will describe these two cross validation method:</p> <hr/> <h1 id="model-evaluation">Model evaluation</h1> <p>In the real world, we usually have a dataset (\(S=\{1,...,n\}\)) including <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">y</code>. We can split the dataset as training set and test set. For a prediction task, a model is training on the training data (<code class="language-plaintext highlighter-rouge">X_train, y_train</code>), then the trained model is tested on the test data (<code class="language-plaintext highlighter-rouge">X_test, y_test</code>). Selecting a training set once randomly could cause selection bias or overfitting, and it does not reflect the real performance of a model on an independent dataset (i.e., an unknown dataset, for instance from a real problem).</p> <p>To solve this problem, we can calculate the average performance of model on iterative datasets. In the figure, I illustrate the 5-fold cross validation for model evaluation. Note that validation fold is test data.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/cross.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Model evaluation. </div> <hr/> <h1 id="model-selection">Model selection</h1> <p>This technique provides a objective way to decide the tuning parameter (hyperparameter in machine learning). Note that, in the model selection, we just split the training set to K folds, while we split the whole dateset in model evaluation (see the figure). When \(K = n\), we call this leave-one-out cross-validation, because we leave out one data point at a time.</p> <p>K-fold cross validation considers training on all folds of data except <em>k</em>th fold, and then test the model on <em>k</em>th fold of data. Then, we repeat this process on other splits, iterating over \(k=1,...,K\). For \(k=1,...,K\), we calculate the total error (calculate the performance, i.e. MSE showing down below) on the test fold:</p> \[e_k(\theta) = \sum_{i\in S^k} (y_i^k - \hat{f}_{\theta}^{-k}(x_i^k))^2\] <p>where \((x_i^k, y_i^k)\) is the data in test fold (\(S^k\)). \(\hat{f}_{\theta}^{-k}\) is the estimated model from \(K-1\) training folds (\(S^{-k}\)) for hyperparameter \(\theta\). Then, we compute the average error over all folds for each hyperparameter \(\theta\):</p> \[CV(\theta) = \frac{1}{n}\sum_{k=1}^K e_k(\theta) = \frac{1}{n}\sum_{k=1}^K\sum_{i\in S^k} (y_i^k - \hat{f}_{\theta}^{-k}(x_i^k))^2\] <p>Finally we can find the optimal hyperparameter \(\hat{\theta}\) where the model has the best performance:</p> \[\hat{\theta}= \underset{\theta\in \{\theta_1,...,\theta_m\}}{\operatorname{argmin}} CV(\theta)\] <p>I recommend you read the paper <code class="language-plaintext highlighter-rouge">A Bias Correction for the Minimum Error Rate in Cross-validation</code><d-cite key="tibshirani2009bias"></d-cite> by Ryan J. Tibshirani and Robert Tibshirani for more theoretical explaination. In this blog, I also used open access lecture notes of <code class="language-plaintext highlighter-rouge">Data mining</code><d-footnote>https://www.stat.cmu.edu/~ryantibs/datamining/lectures/18-val1.pdf</d-footnote> and <code class="language-plaintext highlighter-rouge">Advanced Methods for Data Analysis</code><d-footnote>https://www.stat.cmu.edu/~ryantibs/advmethods/notes/errval.pdf</d-footnote> from CMU Statistics &amp; Data Science.</p> <p>In my github repository <a href="https://github.com/zhiqiangliao/PyCV"><code class="language-plaintext highlighter-rouge">PyCV</code></a>, I have written some python codes for the two types of cross validation. However, those codes are just demos which maybe do not fit the problems you meet. If you need more general python or julia codes, please feel free to contact me. I will see if it is necessary to update the repository.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/norm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Model selection. </div>]]></content><author><name>Zhiqiang Liao</name></author><category term="Research"/><category term="math"/><category term="github"/><summary type="html"><![CDATA[Statistics & Machine learning]]></summary></entry><entry><title type="html">Standardization in Machine Learning</title><link href="https://zhiqiangliao.github.io/blog/2022/standardization/" rel="alternate" type="text/html" title="Standardization in Machine Learning"/><published>2022-10-30T18:40:16+02:00</published><updated>2022-10-30T18:40:16+02:00</updated><id>https://zhiqiangliao.github.io/blog/2022/standardization</id><content type="html" xml:base="https://zhiqiangliao.github.io/blog/2022/standardization/"><![CDATA[<p>Normalization usually helps accelarate convergence of algorithm.</p> <h2 id="0-1-standardization-minmaxscaler">0-1 standardization (MinMaxScaler)</h2> <p>MinMaxScaler standardization is one of the most widely recognized approaches to standardize information. For the variables, the base estimation of that element gets changed into 0, the most extreme worth gets changed into a 1, and each other worth gets changed into a decimal somewhere in the range of 0 and 1. For the responses, the elements can be standardized to have unit \(\|\cdot\|_2\) norm. In many applications, the responses \(y\) also retain the original scales.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Python code
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="p">.</span><span class="nc">MinMaxScaler</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span></code></pre></figure> <figure class="highlight"><pre><code class="language-julia" data-lang="julia"><span class="c"># Julia code</span>
<span class="k">using</span> <span class="n">StatsBase</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">fit</span><span class="x">(</span><span class="n">UnitRangeTransform</span><span class="x">,</span> <span class="n">x</span><span class="x">,</span> <span class="n">dims</span><span class="o">=</span><span class="mi">1</span><span class="x">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">StatsBase</span><span class="o">.</span><span class="n">transform</span><span class="x">(</span><span class="n">scaler</span><span class="x">,</span> <span class="n">x</span><span class="x">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">./</span> <span class="n">norm</span><span class="x">(</span><span class="n">y</span><span class="x">)</span></code></pre></figure> <h2 id="z-score-standardization-standardscaler">Z-score standardization (StandardScaler)</h2> <p>The StandardScaler is used for standardizing scores on the same scale where the variables are standardized to have unit \(\|\cdot\|_2\) norm.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Python code
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span></code></pre></figure> <figure class="highlight"><pre><code class="language-julia" data-lang="julia"><span class="c"># Julia code</span>
<span class="k">using</span> <span class="n">StatsBase</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">fit</span><span class="x">(</span><span class="n">ZScoreTransform</span><span class="x">,</span> <span class="n">x</span><span class="x">,</span> <span class="n">dims</span><span class="o">=</span><span class="mi">1</span><span class="x">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">StatsBase</span><span class="o">.</span><span class="n">transform</span><span class="x">(</span><span class="n">scaler</span><span class="x">,</span> <span class="n">x</span><span class="x">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">./</span> <span class="n">norm</span><span class="x">(</span><span class="n">y</span><span class="x">)</span></code></pre></figure>]]></content><author><name></name></author><category term="Research"/><category term="math"/><category term="code"/><summary type="html"><![CDATA[Statistics & Machine learning]]></summary></entry></feed>