---
layout: post
title:  Cross validation in simulations
date:   2022-11-18
description: Statistics & Computer science
tags: math code
categories: Research
---
Cross validation becomes an increasingly popular method in either theoretical or practical research. There are usually two types of cross validation techniques. 1) The first is to provide a statistical analysis about how accurately a predictive model will perform in practice. 2) The second is to find the optimal hyperparameter or tuning parameter. They usually refer to `model evaluation` and `model selection`, respectively.

I think it is important to clarify the difference between these two types of cross validation techniques. That always confuses beginners, as it confused me when I was learning this technique. I would say the [Wikipedia article](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is not good enough for beginners. Wikipedia refers the second one as nested cross-validation, but it is obvious not clear.

In effect, the basic idea of them are the same: out-of-sample testing. Next, I will describe these two cross validation method:

***

# Model evaluation

In real world, we usually have a dataset including `X` and `y`. We can split the dataset as training set and test set. For a prediction task, a model is training on a given dataset or training data (`X_train, y_train`), then the trained model is tested on the test data (`X_test, y_test`). Selecting a training set once randomly could cause selection bias or overfitting, and it does not reflect the real performance of a model on an independent dataset (i.e., an unknown dataset, for instance from a real problem).

To solve this problem, we can calculate the average performance of model on iterative datasets. In the figure, I illustrate the 5-fold cross validation for model evaluation. Note that I prefer to the term of testing set than validation fold.

<hr>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/cross.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Model evaluation.
</div>

<hr>


# Model selection

This technique provides a objective way to decide the hyperparameter (tuning parameter in computer science). Note that, in model selection, we just split the training set to 5 folds, while we split the whole dateset in model evaluation. Next, in each split, we train the model on all 4 folds data and test the model on one fold data for one hyperparameter. Usually, we test (calculate the score or performance, i.e. MSE) the model on series of hyperparameters, and then find the optimal hyperparameter where the model has best performance. Then, we repeat this process on other splits. Finally, the cross validation hyperparameter is computed by averaging the five best hyperparameters on the 5 splits.

I recommend you read the article `A Bias Correction for the Minimum Error Rate in Cross-validation` by Ryan J. Tibshirani and Robert Tibshirani for more theoretical explaination. In my github repository [PyCV](https://github.com/zhiqiangliao/PyCV), I have written some python codes for the two types of cross validation. However, those codes are just demos which may not suit your problems. If you need more general python or julia code, please feel free to contact me. I will see if it is necessary to update the repository.

<hr>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/norm.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Model selection.
</div>

<hr>

